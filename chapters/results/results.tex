\chapter{Results}
\label{chap:Results}

After application of all cuts to the analysis data set (the `$90 \%$ sample' described in \SectionRef{sec:Analysis:Blinding}) there are no surviving events and hence no neutrino candidates. Therefore there is no evidence found for a flux of ultra-high energy (UHE) neutrinos detected in the TestBed, hence a model independent limit can be calculated. Upon relaxing the anthropogenic geometry and goodTimes cuts 4 periods of heightened solar activity, caused by solar flares, were identified and detected in the TestBed. The events associated with this solar activity are discussed in \SectionRef{sec:Results:Solar-Flare}, followed by the calculatation of an UHE neutrino limit.


\section{Solar flare events}
\label{sec:Results:Solar-Flare}

The TestBed analysis detected $\sim 6 \times 10^{3}$ events, that reconstructed close to the location of the sun, associated with 4 solar flare events. Since these events reconstructed to locations above the ice, they failed the geometric cuts in the TestBed analysis. These events are, however, of interest since they are consistent with originating from a distant source and can be used to assess the performance of signal reconstruction. The first solar flare event observed occurred on $15^{\mbox{th}}$ February 2011, the remaining 3 events were in 2012 on $5^{\mbox{th}}$ March, $7^{\mbox{th}}$ March and $21^{\mbox{st}}$ of November respectively.



\begin{figure} [htpb]
  \subfloat[Power above baseline]{\includegraphics[width=0.48\textwidth]{chapters/results/SolarFlare-VPol-Ant4.pdf}}\hfill
  \subfloat[Power above baseline]{\includegraphics[width=0.48\textwidth]{chapters/results/SolarFlare-VPol-Ant4-Zoom.pdf}}\\
  \subfloat[GOES X-ray flux]{\includegraphics[width=0.48\textwidth]{chapters/results/SolarFlare-GOES.pdf}}\hfill
  \subfloat[GOES X-ray flux]{\includegraphics[width=0.48\textwidth]{chapters/results/SolarFlare-GOES-Zoom.pdf}}
  \caption{The solar flare event on $15^{\mbox{th}}$ February 2011 as observed in the TestBed. The average frequency domain power in one VPol antenna, taken over a minute period, is compared to a baseline produced from a nearby run in (a) and (b). This is compared to the X-ray flux measured by the GOES satellite system \cite{2009SPIE.7438E...1C} (c) and (d), divided into two bands: $(0.5 - 4.0) \times 10^{-10} \meter$ band (blue), and $(1.0 - 8.0) \times 10^{-10} \meter$ band (red). (a) and (c) show the these quantities over the course of a day, whereas (b) and (d) focus on the period of largest activity.}  
  \label{fig:Results:Solar-Flare:Frequency}
\end{figure}

\FigureRef{fig:Results:Solar-Flare:Frequency} shows, for one solar flare period, the averaged frequency domain power relative to a baseline taken from a nearby run, and is typical of the 4 vertically polarise (VPol) antennas during this period. Broadband power excesses are seen in the 4 VPol antennas, which seem to be associated with increased X-ray fluxes as observed by the GOES satellite system \cite{2009SPIE.7438E...1C}. There is a clear coincidence of elevated power levels in the VPol antennas with the largest peak in the GOES X-ray flux at 120 minutes past midnight, however the correlation with subsequent peaks in the GOES flux is not so clear. This is typical of the periods of heightened solar activity that correlate with events observed in the TestBed. Although the small scale time structure of the GOES data is not fully replicated in the recorded events all of these periods do see elevated solar activity and events that reconstruct back to the sun's location in the sky. Inspection of the events' waveforms did not show obvious impulsive signals, however these events exhibit similar waveforms in multiple antennas resulting in non-thermal correlations and the ability to reconstruct their source. 

An attempt was made to use these events to determine the reconstruction's accuracy, since the sun's position can be calculated given the time the events were recorded. \FigureRef{fig:Results:Solar-Flare:Phi} shows the result of comparisons between the azimuth of the sun and reconstructed events. 

\begin{figure}
  \subfloat[Reconstructed $\phi$ versus sun $\phi$]{\includegraphics[width=0.48\textwidth]{chapters/results/solarFlare-phiPhi.pdf}}\hfill
  \subfloat[$\phi$ residuals]{\includegraphics[width=0.48\textwidth]{chapters/results/solarFlare-deltaPhi.pdf}}
  \caption{Comparison of reconstructed azimuth of solar flare events with the calculated position of the sun.}
  \label{fig:Results:Solar-Flare:Phi}
\end{figure}

\section{Live Time}
\label{sec:Results:Live-Time}

In order to calculate the sensitivity to a neutrino flux the integrated live time for the analysis data set must be calculated. Due to limitations in the TestBed DAQ there is a period after a trigger is asserted, and event readout initiated, for which it is not possible to form another trigger. This is due in part to the finite digitisation and readout time of the LAB3 digitisers utilised in the TestBed, however this corresponds to only $50 \micro \second$ per event \footnote{At a trigger rate of $2 \hertz$ this would correspond to a fractional dead-time of $0.01 \%$.}. The majority of the dead-time in the TestBed is understood to be associated with event readout through the DAQ firmware and software, and is rate dependent.

The TestBed electronics monitors dead-time via a $10 \mega \hertz$ clock. The dead-time is calculated by taking the number of clock cycles for which the trigger is unavailable in each GPS second, which can be scaled by the clock frequency to recover the fractional dead-time for each second of operation. The total live time for the analysis data set can then be calculated by taking the difference between the number of GPS seconds for which the TestBed operated and the sum of independent dead times recorded. 

Having obtained an integrated live time for the entire analysis data set a final adjustment must be made to take into account the goodTimes cut described in \SectionRef{sec:Analysis:Anthropogenic-Cuts:Good-Times} and periods for which a significant fraction of events are rejected due to data quality issues. The goodTimes cut masks off periods of the year that are associated with elevated levels of human activity and non-thermal events passing analysis cuts. All runs that are masked off by this cut have their integrated live time removed from the final live time measurement. Similarly any days of the year that have a sizeable fraction of events suffering from data quality issues have their integrated live time removed from the total. 


The fractional live time during each day of the year, as well as the resulting integrated live time are shown in \FigureRef{fig:Results:Integrated-LiveTime}. The integrated live time for 2011 shows a number of features. The first two are the lack of live time between days 0-30 and 335-365 caused by the good times cut, which masks off all events from the first and last month of the year. The same feature is present in the 2012 integrated live time. There is also a period between days 260-270 in 2011 which is masked off, again by the good times cut, but this time due to the large number of events passing the thermal cuts during this period. In 2012 there are other periods in which there is no accumulated live time, notably between days 95-130 and from day 260 onward. The former period is due to a data quality issue which rendered waveform data unusable. The latter corresponds to a large period of time in which there were elevated trigger rates in the TestBed, which corresponds to the drop in fractional live time in \FigureRef{fig:Results:Integrated-LiveTime} (d), and large numbers of events passing thermal cuts. This lead to this large period of data being rejected by the good times cuts.

\begin{figure}
  \subfloat[Live time for 2011]{\includegraphics[width=0.48\textwidth]{chapters/results/LiveTime2011.pdf}}\hfill
  \subfloat[Live time for 2012]{\includegraphics[width=0.48\textwidth]{chapters/results/LiveTime2012.pdf}}\\
  \subfloat[Live time for 2011]{\includegraphics[width=0.48\textwidth]{chapters/results/FractionalLiveTime2011.pdf}}\hfill
  \subfloat[Live time for 2012]{\includegraphics[width=0.48\textwidth]{chapters/results/FractionalLiveTime2012.pdf}}
  \caption{Integrated live time for 2011 and 2012. Three lines are shown for each year: live time for all runs, live time for all runs with the majority of events passing data quality checks and finally live time for all runs passing both data quality and goodTimes criteria. Fractional live time is also shown for all runs (red) and for those passing data quality and goodTimes criteria (shaded magenta).}
  \label{fig:Results:Integrated-LiveTime}
\end{figure}

After accounting for data quality issues and applying the goodTimes criteria the integrated live time is 167.4 days for 2011 and 50.2 days for 2012. The resulting live time for the full analysis data set is calculated to be 217.6 days.

\section{Effective area}
\label{sec:Results:Effective-Area}

The effective area is calculated for the TestBed from simulated neutrino data sets produced using AraSim as follows. Firstly the effective volume, $V_{eff}$, is calculated for each energy bin using a large sample of simulated events ($\sim 10^{6}$):

\begin{equation}
  V_{eff} = \frac {V_{cylinder}}{N} \sum_{i=1}^{N_{passed}} w_{i}
\end{equation}

\noindent where $V_{cylinder}$ is the volume of a cylinder centred on the detector in which events are simulated, $N$ is the total number of events thrown and $\sum_{i=1}^{N_{passed}} w_{i}$ is the sum of weights for triggered events. By throwing neutrinos within a cylinder around the detector the computational burden of producing simulation data sets is greatly reduced. The data sets analysed were produced using cylinders of $3 - 4 \kilo\meter$ radii, with larger radii chosen for higher energy neutrinos, well above the attenuation length of radio waves in the ice \cite{Allison:2014kha}. 

The effective volume is then converted into an effective area, $A_{eff}$, by using the approximation:

\begin{equation}
  A_{eff} \approx \frac{V_{eff}}{l_{int}}
\end{equation}

\noindent where $l_{int}$ is the neutrino interaction length in ice \cite{Allison:2014kha}. 

\section{Neutrino flux limit}
\label{sec:Results:Limit}

An upper limit can be placed on the neutrino flux using the following equation:

\begin{equation}
  E_{\Pnu} F(E_{\Pnu}) \leq \frac{N_{90}}{4\pi A_{eff}(E_{\Pnu}) T_{live} \varepsilon(E_{\Pnu}) \ln(10)}
\end{equation}

\noindent where $T_{live}$ is the total live time of the TestBed analysis data set, $\varepsilon(E_{\Pnu})$ is the energy dependant analysis efficiency, $N_{90}$ is the $90\%$ upper confidence limit on the number of neutrino events excluded by the analysis, which is taken as 2.3 from Poisson statistics, and the factor $\ln(10)$ accounts for log-scale energy binning \cite{Allison:2014kha}. The flux is defined as follows:

\begin{equation}
  F(E_{\Pnu}) = \frac {dN}{dE_{\Pnu} d\Omega dt dA} 
\end{equation}

The live time and effective area are calculated as discussed in \SectionRef{sec:Results:Live-Time} and \SectionRef{sec:Results:Effective-Area}, and the analysis efficiency taken from application of cuts to simulated neutrino events shown in \FigureRef{fig:Analysis:Cut-Results:Efficiency}. 



\begin{figure}[htpb]
  \includegraphics[width=\hugefigwidth]{chapters/results/LimitPlotr2801.pdf}
  \caption{TestBed analysis neutrino flux limit from this analysis `UCL 218', along with limits from ANITA \cite{PhysRevD.82.022004}, Auger \cite{Abraham:2009eh}, RICE \cite{PhysRevD.73.082002} and IceCube \cite{PhysRevD.88.112008}. The shaded band indicates a range of flux predictions from \cite{Kotera.2010} using a variety of assumptions about sources and production mechanisms.}
  \label{fig:Results:Limit-Plot}
\end{figure}

The resulting UHE neutrino flux limit is shown in \FigureRef{fig:Results:Limit-Plot} along with limits from other experiments and a range of theoretical predictions. In addition to the limit calculated from this analysis of two years of TestBed data a projected sensitivity is shown for ARA-37 using a modified simulation \footnote{At the time of analysis data sets were not available for neutrino energies above $10^{20} \eV$ for the projected ARA-37 sensitivity.} . A number of factors lead to the much improved sensitivity reached by the full array. First and foremost is the increase in number of stations from one to thirty seven, each acting as a stand-alone neutrino detector, operated over a longer period. Deployment of stations at $\sim 200 \meter$ depths has been demonstrated over the 2012-2013 season and all further stations are expected to deployed at similar depths. By placing the antennas well below the firn layer of ice (which extends $\sim 150 \meter$ below the surface in the vicinity of the south pole), in which the changing index of refraction leads to ray-bending and shadowing effects, an increase in effective volume and area per station of the order $\sim 3$ over the TestBed is expected.

The expected level of anthroprogenic backgrounds is also anticipated to reduce greatly as the vast majority of future stations will be much further from the south pole station and associated human activity. In addition the receive antennas being placed much deeper in the ice will lead to a reduction in the signal size of above ice noise sources in each station. As well as the reduction in signal size the increased number of receive antennas, from four per polarisation to eight, is expected to significantly improve background identification and rejection. Anthropogenic signal removal via masking off noisy periods of time, which is performed via the goodTimes cut described in \SectionRef{sec:Analysis:Anthropogenic-Cuts:Good-Times}, leads to large periods of time being removed from the analysis sample and greatly reduces the total detector live time in this analysis. The expected reduction in number anthropogenic signals associated with location will be complemented with better characterisation of such signals and removal as the radio environment becomes better understood.

The re-designed data acquisition electronics and software (DAQ) will also lead to improvements in sensitivity. The updated DAQ allows for triggered event buffering such that the expected dead time associated with event readout described in \SectionRef{sec:Results:Live-Time} will be effectively nil. 







