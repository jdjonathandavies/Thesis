\chapter{Analysis}
\label{chap:Analysis}


%Analysis strategy
The TestBed operated with very little downtime for the duration of 2011 and 2012 with the full data set archived to disk at the south pole. A limited bandwidth satellite link was available to transfer approximately $10 \%$ of this data back to the northern hemispere data warehouse with a latency of a few days. This `filtered' data set, which was randomly chosen, was used to asses operation and conditions in the TestBed such that significant data quality and downtime issues could be addressed during the winter season. The disks containing the full data set from 2011 and 2012 were hand carried back to the northern hemisphere during the 2011-2012 and 2012-2013 summer seasons and subsequently stored in the data warehouse, hosted at the University of Wisconsin. This analysis is based on that data set.

A blinding strategy was agreed by the experiment to avoid bias of the analyses. The `burn sample' of data, which analysers use to train their analysis and cuts, consists of all software triggered (or minbias) events, all calibration pulser events and $10 \%$ of the remaining RF triggers. The remaining $90 \%$ of RF triggers were stored separately in the data warehouse to be analysed once unblinding of the analysis was approved by members of the experiment. 

\section{Description of Data}
\label{sec:Analysis:Data}


%Description of data collected





\section{Simulated Neutrino Data}
\label{sec:Analysis:MC-Data}
%Description of Monte Carlo data and calibration data



\section{Calibration Data}
\label{sec:Analysis:Calibration-Data}


\section{Continuous Wave Removal}
\label{sec:Analysis:CWRemoval}

%CW Filtering


%CSW Reconstruction

\section{Event Reconstruction}
\label{sec:Analysis:Reconstruction}







%Thermal rejection cuts



