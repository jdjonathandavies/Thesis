\chapter{TestBed Data Analysis}
\label{chap:Analysis}

The TestBed is a prototype installed with the intention of showing that a much larger detector will be able to reach the necessary sensitivity to make the first observation of ultra high energy (UHE) neutrinos. During the course of 2011 and 2012 the TestBed recorded around 480 million events. Simulations of the TestBed and constraints provided by a number of experiments inform us that very few, if any, of these events will contain signals caused by neutrino interactions in the ice. 

The analysis described in this chapter will show that a single array of radio antennas can be used to reach a reasonable sensitivity to UHE neutrino fluxes that, under realistic assumptions, scales to a projected world leading sensitivity for the full 37 station detector.

\section{Analysis approach}
\label{sec:Analysis:Analysis-Approach}

The TestBed is the first ARA station deployed in the ice around the geographic South Pole. This analysis of the data recorded over its 2 year operating period aims to demonstrate that it is possible to identify neutrino-induced signals within this data set. Almost all of the recorded events will not contain neutrino signals and are therefore considered backgrounds to this analysis. In order to identify the small number of signal events it is necessary to reduce the expected background levels to of order $0.1$ events remaining after analysis. This requires background rejection levels better than 1 event in a billion passing, which is a considerable challenge.

The expected sources of background events falls into two categories: firstly random fluctuations in the radio frequency (RF) environment caused by the thermal conditions in and around the TestBed, and secondly RF noise associated with human activity and infrastructure in the vicinity of the station. These backgrounds, which are referred to as thermal noise and anthropogenic signals from this point forwards, have differing characteristics that can be used to separate them. Some anthropogenic sources are expected to produce signals that follow closely the expectations of neutrino-induced events. An important goal of any analysis of TestBed data is to show that this background is at a manageable level that will not impact heavily on a much larger array's sensitivity.

These categories of events have very different properties so an analysis approach is taken that aims to remove them in two stages. The first stage is to remove thermal noise events by requiring coherence between signals in multiple antennas consistent with a common physical source location. Due to the random nature of thermal events it is possible to use reconstruction and coherence tests to remove them from the data set, leaving only directional events. The majority of the remaining events are expected to be of anthropogenic origin associated with infrastructure and human activity around the South Pole Station, hence a second stage is concerned with removing such backgrounds. These signals are likely to exhibit both spatial and temporal correlations not expected from a sample of neutrino events. Therefore it is possible to remove such anthropogenic signals by rejecting reconstructed source locations associated with infrastructure or repeated signals and finally by removing events from time periods associated with high levels of human activity.

In addition to directional anthropogenic signals another category of events is expected in the TestBed resulting from the operation of narrow-band communications equipment. Events containing such carrier wave (CW) signals are likely to pass coherence tests intended to remove thermal events but will not reconstruct to their origin, making spatial correlation tests of limited use. Reconstruction of incident signals relies on using time offsets between signals derived from the correlation between them. For CW signals period ambiguities give rise to a number of possible offsets between a pair of antennas, each as likely as the other, resulting in an inability to clearly identify the source location. Some fraction of these events will miss-reconstruct to physical locations in the ice so it is necessary to remove them by other means. By considering the frequency content of such events it will be shown that it is possible to separate these signals from neutrino-like events due to their narrow-band nature.



In \SectionRef{sec:Analysis:Blinding} and \SectionRef{sec:Analysis:Data-types} the data sets and blinding strategy for this analysis will be outlined. A series of tools were developed for use in this analysis in order to reconstruct events, identify those with coherent signals between multiple antennas and to remove CW contamination from the data set. These tools will be described in \SectionRef{sec:Analysis:Analysis-Tools}. A series of cuts are then identified and chosen to separate signals from the various categories of background events. Firstly data quality cuts are described in \SectionRef{sec:Analysis:Data-Quality-Cuts}, then the thermal noise and anthropogenic signal cuts are described in \SectionRef{sec:Analysis:Thermal-Cuts} and \SectionRef{sec:Analysis:Anthropogenic-Cuts} respectively.

Any events that pass all of these cuts are considered to be neutrino candidates. A deliberate choice was made to omit any signal shape cuts in order to minimise dependencies on theoretical calculations of Askaryan signals and assumptions made about radio propagation in the ice. 




\section{Blinding}
\label{sec:Analysis:Blinding}
The TestBed operated with very little downtime for the duration of 2011 and 2012 with the full data set archived to disk at the South Pole. A limited bandwidth satellite link was available to transfer approximately $10 \%$ of this data back to the northern hemisphere data warehouse with a latency of a few days. This `filtered' data set, which was randomly chosen, was used to assess operation and conditions in the TestBed such that significant data quality and downtime issues could be addressed during the winter season. The disks containing the full data set from 2011 and 2012 were hand carried back to the northern hemisphere during the 2011-2012 and 2012-2013 summer seasons. The data was subsequently stored in the data warehouse, hosted at the University of Wisconsin, and used as the basis of this analysis.

A blinding strategy was agreed by the experiment to limit any bias in analyses performed on the data. This was implemented by splitting the analysis data set into to two samples: a `burn sample' intended to be used for training the analysis and associated cuts, and a `$90 \%$ sample' containing the remaining events. Once the experiment was in agreement with the methods, tools and cuts implemented on the `burn sample' of data the analysis was unblinded and run over the `$90 \%$ sample', with any events passing the cuts considered as neutrino candidates.

\section{Data types}
\label{sec:Analysis:Data-types}

There are two types of event recorded and made available for analysis:

\begin{itemize}

\item \textbf{RF triggered events} - events in which the trigger condition is met requiring 3 of 16 antennas to have passed a power threshold within a $110 \nano \second$ window.
\item \textbf{Minimum bias events} - recorded at a rate of approximately $0.5 \hertz$ these events are force triggers where the RF trigger condition has not been met and are a reflection of the background conditions in which the TestBed was operating.
\end{itemize}

The RF triggered events are further sub-divided into two classes:

\begin{itemize}
\item \textbf{Calibration pulser events} - these events are RF triggered events that are identified as containing signals from a number of calibration antennas by timing information. 
\item \textbf{Non-calibration pulser events} - the remaining set of RF triggered events after calibration pulser events have been removed.
\end{itemize}



The non-calibration pulser events recorded by the TestBed form the majority of the data set and are those to be analysed and assessed to see whether they contain neutrino induced signals. Minimum bias events were recorded at regular intervals by the DAQ system in order to provide a picture of the radio environment during the course of data taking. As they do not require a physics trigger condition these events are in the main thermal in nature. There are, however, periods for which repetitive or long duration anthropogenic noise sources are active contaminating this thermal sample of events.

During the course of data taking two calibration sources were active: a VPol calibration pulser in 2011, and a HPol calibration pulser in 2012. The pulsers were fired at a rate of $1 \hertz$ using the DAQ's GPS PPS as a trigger. The result is that these events are identifiable via trigger timing information and removed from the `$90 \%$ sample'. These calibration pulser events were used in training the reconstruction methods as well as a number of the cuts used in this analysis. Being the only controlled in-ice source of impulsive radio signals calibration pulser events are used as a proxy for neutrino induced signals and provide an invaluable cross-check for analysis tools and cuts designed to identify impulsive signals.

The two data samples defined in the blinding strategy were comprised of a combination of the event types described above:

\begin{itemize}

\item \textbf{burn sample} - Sample of events intended for analysis tools and cut development. Comprised of all calibration pulser events, all minimum bias events and $10 \%$ of non-calibration pulser events.
\item \textbf{$90 \%$ sample} - The remaining $90 \%$ of non-calibration pulser events not made available in the burn sample.

\end{itemize}

A third sample of events consisting of simulated neutrino signals were produced using the official ARA simulation AraSim \cite{AraSim}. This sample was produced with fixed incident neutrino energies in half decade intervals between $10^{17} \eV$ and $10^{21} \eV$ and was used to inform cuts and help with development of analysis tools such as source reconstruction. 

AraSim simulates neutrino signals in three stages: the neutrino propagation and interaction, RF signal transmission through the ice to the TestBed antennas and finally the output of the digitisation and triggering paths within the TestBed detector. The resulting data set contains digitised waveforms that would be expected to be recorded by the TestBed given knowledge of the TestBed detector and neutrino interactions. 

Neutrinos are produced with isotropic arrival directions within a uniform volume around the centre of the TestBed. Each event is simulated independently and assigned a weight representing the probability that the neutrino would have reached the interaction point without being absorbed in the Earth. The shower development and RF Cherenkov emission are modelled before signals are propagated to the TestBed using a ray-tracing algorithm to determine the path taken through ice which, due to the changing index of refraction, leads to ray bending effects resulting in zero, one or two rays arriving at the antennas. Finally the signals are propagated through a model of the triggering and digitisation chains within the TestBed, which are based upon measurements taken in the laboratory ahead of installation in the ice.

In addition to simulating the expected neutrino signals thermal noise is added to the events. The noise model is derived from minimum bias events recorded in the TestBed and tuned to match the observed noise levels and trigger rates. 

\section{Analysis tools}
\label{sec:Analysis:Analysis-Tools}

The TestBed records waveforms from both vertically polarised (VPol) and horizontally polarised (HPol) antennas for each event and these polarisations are treated separately in analysis. This is in part due to the differences in signal chain and hence noise levels between the two. In addition it is expected that the majority of anthropogenic signals will be limited to a single polarisation. Once initial data-quality checks have been applied to the events they are split into two `half events', one comprising of the signals in the VPol antennas and the other similarly from HPol antennas, that are propagated through the remaining cuts in order to assess whether or not they are to be considered a neutrino event.

This section will outline the analysis tools that are used on the analysis data set. They were designed to be used with waveforms of a single polarisation and each of the tools described are applied to both the VPol and HPol antennas separately.


\subsection{Event Reconstruction}
\label{sec:Analysis:Reconstruction}

Directional reconstruction of incident radio signals recorded in the TestBed is a powerful tool with which to identify and remove backgrounds, whilst providing invaluable information about possible neutrino events. Interferometric techniques have been successfully used in similar experiments, for example the ANITA experiment \cite{HooverThesis} \cite{AbbyThesis} \cite{MattThesis} \cite{AndresThesis}, that also record high fidelity radio waveforms. There are, however, a number of obstacles to robust reconstruction using interferometry in the TestBed. 

The small number of antennas (4 deep antennas in each polarisation) limit the measured strength of any preferred source direction, which would be re-enforced with more antennas. The varying index of refraction in the firn layer (typically extending $\sim 150 \meter$ into the ice close to the South Pole) causes ray-bending effects that distort the true arrival direction. And finally the uncertainty in the deployed positions of the receive and calibration antennas in the ice can further complicate reconstruction. 


A new reconstruction method was developed for this analysis, based upon calculating timing offsets between waveforms that maximise correlation. This is achieved by creating a coherently summed wave (CSW) where individual antenna waveforms, scaled by the total number of waveforms, are added offset relative to one another. These offsets are calculated in a manner such that the correlation between the resulting CSW and the original waveforms is maximised, as measured by the normalised cross-correlation $C_{1,2}$:


\begin{equation}
  C_{1,2}(\Delta t) = \frac{\psi_{1} \star \psi_{2}}{\sigma_{1} \sigma_{2}}
  \label{eq:analysis:Reconstruction:Normalised-Cross-Correlation}
\end{equation}

\noindent where $\psi_{1/2}$ are the time domain waveforms, $\Delta t$ the time offset between them and $\sigma_{1/2}$ are the waveform's RMS. For discretely sampled waveforms containing a limited number of samples the cross-correlation $\psi_{1} \star \psi_{2}$ is given by:

\begin{equation}
  \psi_{1} \star \psi_{2} (\Delta t) = \sum_{n}^{N} V_{1}(t_{n}) V_{2}(t_{n}-\Delta t)
  \label{eq:analysis:Reconstruction:Discrete-Cross-Correlation}
\end{equation}

\noindent where the voltages $V_{1/2}$ outside of waveform window are taken to be zero. In order to improve the timing resolution obtained from correlations the individual antenna waveforms are up-sampled, using interpolation to an evenly sampled time-base, so that the time between adjacent samples is $0.25 \nano \second$ and the corresponding offsets $\Delta t$ used in \EquationRef{eq:analysis:Reconstruction:Discrete-Cross-Correlation} are the same. A simple algorithm is implemented to find the set of timing offsets that maximise the correlation between each waveform and the resulting CSW. The timing differences between pairs of antennas found using this method are shown in \FigureRef{fig:analysis:Reconstruction:CSW-DeltaT} for a selection of calibration pulser events a width of $\sim 150 \pico \second$. 

\begin{figure}[htpb]
  \subfloat[Antenna 1 and Antenna 2]{\includegraphics[width=0.49\textwidth]{chapters/analysis/DeltaT01-CP-VPol.pdf}}\hfill
  \subfloat[Antenna 1 and Antenna 3]{\includegraphics[width=0.49\textwidth]{chapters/analysis/DeltaT02-CP-VPol.pdf}}\\
  \subfloat[Antenna 1 and Antenna 4]{\includegraphics[width=0.49\textwidth]{chapters/analysis/DeltaT03-CP-VPol.pdf}}\hfill
  \caption{CSW measured time offsets between pairs of VPol antennas for calibration pulser events.}
  \label{fig:analysis:Reconstruction:CSW-DeltaT}
\end{figure}


The VPol calibration pulser signal, which was used during 2011, is a very bright source causing compression in some of the digitised waveforms and resulting in the main peak and side lobes being of a similar size. When the CSW is formed either the main peak or side lobes of the waveforms are lined up, leading to a double peak structure in the time difference between antenna's 1 and 3. 

In \FigureRef{fig:analysis:Reconstruction:CSW-Example} the calculated time differences are applied to offset individual waveforms. These offset waveforms are summed to form a CSW also shown for a calibration pulser event, noise event and simulated neutrino event. The CSWs formed in this manner differ significantly between thermal noise waveforms and signal type waveforms (i.e. the calibration pulser and simulated neutrino events). Two main features are apparent: firstly the size of the peak in the coherently summed wave is much greater for signal type events, and secondly the resulting CSW looks very similar to the shape of the individual waveforms for signals of interest, which is not the case for noise events. The similarity of signals in impulsive events, such as the calibration pulser and simulated neutrino events, leads to constructive interference in the CSW. The noise, on the other hand, adds destructively leading to an increased  signal to noise ratio in the former. The larger signal sizes and correlation between individual antennas and the CSW can be used to aid discrimination between signals and backgrounds in an analysis.


\begin{figure}[htpb]
  \subfloat[Individual waveforms calibration pulser event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-Coherent-Waveforms-VPol-CP-Med.pdf}}\hfill
  \subfloat[CSW calibration pulser event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-CSW-VPol-CP-Med.pdf}}\\
  \subfloat[Individual waveforms noise event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-Coherent-Waveforms-VPol-Noise-Med.pdf}}\hfill
  \subfloat[CSW noise event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-CSW-VPol-Noise-Med.pdf}}\\
  \subfloat[Simulated neutrino event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-Coherent-Waveforms-VPol-Nu-Med.pdf}}\hfill
  \subfloat[Simulated neutrino event]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CSW-CSW-VPol-Nu-Med.pdf}}
  \caption{(a), (c) and (e) show individual antenna waveforms aligned in time. (b), (d) and (f) show the resulting coherently summed wave.} 
  \label{fig:analysis:Reconstruction:CSW-Example}
\end{figure}


The timing difference between pairs of antennas holds information about the arrival direction of the radio signal and are checked against those calculated from a simple ice model. The model takes the index of refraction to be a constant, equal to the measured value at the average depth of the borehole antennas. A pseudo-$\chi^{2}$ is computed for a series of trial source locations in 1 degree bins in $\theta$, $\phi$ and logarithmically spaced bins in radial distance $R$:

\begin{equation}
  \mbox{pseudo-}\chi^{2} = \sum_{i} \frac{(\Delta t_{1,i}^{meas} - \Delta t_{1,i}^{exp})^{2}}{\sigma^{2}}
  \label{eq:analysis:Reconstruction:ChiSq}
\end{equation}


\noindent  where $\Delta t_{1,i}^{meas}$ is the measured offset between antenna $1$ and antenna $i$, $\Delta t_{1,i}^{exp}$ is the calculated offset expected from a trial source location and $\sigma$ is taken to be $1 \nano \second$ (which is similar to the timing uncertainty expected given uncertainty on the antenna positions). The reconstructed location is that which minimises pseudo-$\chi^{2}$ and hence corresponds to the most likely physical location given the measured time offsets. Calculating pseudo-$\chi^{2}$ values in all possible $\theta$, $\phi$ and $R$ bins is computationally intensive and a time consuming process, so an algorithm was developed to only search a subset and identify the best fit location.


 This method has the benefit of using the rich information contained within the digitised waveforms (correlation techniques result in precision of $\sim 150 \pico \second$ resolution in timing differences between pairs of antennas) as well as providing a parameter that describes the goodness of fit in pseudo-$\chi^2$ upon which a cut can be placed. As thermal signals will have random measured offsets between antennas the preferred source location will, in general, have a relatively large pseudo-$\chi^2$ value associated with it, thus a requirement for good reconstruction will additionally reject a large number of thermal events.

\begin{figure}[htpb]
  \subfloat[Pseudo-$\chi^{2}$ map of calibration pulser event]{\includegraphics[width=0.48\textwidth]{chapters/analysis/ChiSqMap-VPol-CP.pdf}}\hfill
  \subfloat[Pseudo-$\chi^{2}$ map of noise event]{\includegraphics[width=0.48\textwidth]{chapters/analysis/ChiSqMap-VPol-Noise.pdf}}\\
  \subfloat[Pseudo-$\chi^{2}$ map of simulated neutrino event]{\includegraphics[width=0.48\textwidth]{chapters/analysis/ChiSqMap-VPol-Nu.pdf}}
  \caption{Reconstruction maps of calculated pseudo-$\chi^{2}$ values for the same calibration pulser event in \FigureRef{fig:analysis:Reconstruction:CSW-Example}. The circles in (a) and (c) indicate the true source location.}
  \label{fig:analysis:Reconstruction:CSW-ChiSq-Example}
\end{figure}

\FigureRef{fig:analysis:Reconstruction:CSW-ChiSq-Example} shows reconstruction maps for a series of event types, consisting of calculated pseudo-$\chi^{2}$ values for points on the surface of a sphere at the same radial distance as the best fit location. CSW reconstruction is attempted for each polarisation in every event, with the best fit location as well as corresponding pseudo-$\chi^{2}$ value recorded. \FigureRef{fig:analysis:Reconstructed:CSW-Residuals} shows the residuals (measured minus expected) for azimuth and elevation angle for an ensemble of calibration pulser events and for neutrino events simulated with AraSim. Reconstruction of azimuth and elevation is $\sim 1^{\circ}$, although the ray-bending effects in the ice cause elevation angles to be systematically shifted. 


\begin{figure}[htpb]
  \subfloat[Simulated neutrino reconstruction azimuth]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Phi-Sim.pdf}}\hfill
  \subfloat[Simulated neutrino reconstruction elevation]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Theta-Sim.pdf}}\\
  \subfloat[Calibration pulser reconstruction azimuth]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Phi-CP.pdf}}\hfill
  \subfloat[Calibration pulser reconstruction elevation]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Theta-CP.pdf}}\\
  \subfloat[Calibration pulser reconstruction azimuth]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Phi-CP-HPol.pdf}}\hfill
  \subfloat[Calibration pulser reconstruction elevation]{\includegraphics[width=0.44\textwidth]{chapters/analysis/CSW-Reco-Residuals-Theta-CP-HPol.pdf}}
  \caption{Residuals for reconstructed source direction azimuth ($\phi$) and elevation ($\theta$). For simulated neutrino events the source location is taken to be the neutrino interaction point. Due to ray-bending effects a correction factor is applied for simulated neutrino events to translate the reconstructed elevation angle to the line of sight to the source.}
  \label{fig:analysis:Reconstructed:CSW-Residuals}
\end{figure}


The ray-bending becomes more pronounced for sources at large radial distances from the TestBed and, as there is more target volume at these large distances, the majority of simulated neutrino events  suffer from these shifts. This effect is shown in \FigureRef{fig:analysis:Reconstruct:AraSim-Theta-Correction} where events coming from elevation angles close to horizontal are reconstructed above their origin. It is possible to calculate a transfer function to correct the shift in reconstructed elevation angle between true and reconstruction source location, which is also shown. This is an effective mapping between the ice models used for simulation of RF propagation in the ice and used in the reconstruction. The elevation residuals for simulated neutrino events in \FigureRef{fig:analysis:Reconstructed:CSW-Residuals} have such a correction applied and show that the systematic shift is successfully removed.

\begin{figure}[htpb]
  \subfloat[Interaction point $\theta$ versus reconstructed $\theta$]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CSW-Reco-Theta-Theta-Sim-Large.pdf}}\hfill
  \subfloat[Reconstructed $\theta$ correction]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CSW-Reco-Theta-Fit-Sim-Large.pdf}}
  \caption{Reconstruction of simulated neutrino elevation angles. In (a) the neutrino interaction point $\theta$ is shown as a function of all reconstructed angles, which exhibits strong ray-bending effects close to and above horizontal angles ($\theta > 0 $). In (b) a profile is taken for events that reconstruct downward ($\theta < 0 $) and a correction function fitted to the data.}
  \label{fig:analysis:Reconstruct:AraSim-Theta-Correction}
\end{figure}


\subsection{Carrier wave removal}
\label{sec:Analysis:CWRemoval}

The frequency band for the TestBed ($150 - 850 \mega \hertz$ for VPol and $250 - 850 \mega \hertz$ for HPol) contains a number of frequencies used for communications at the South Pole. A particularly strong carrier signal used by the South Pole Station at $450 \mega \hertz$ is removed by a notch filter placed between the antenna output and low noise amplifiers down-hole, but a number of other transmission frequencies remain unfiltered. Due to the TestBed's remote location and the relatively low levels of human activity (which are largely restricted to the summer season) CW signals are not present in the vast majority of RF triggers and minimum bias events. However, the presence of CW in an event can cause miss-reconstruction of the incident signal and, in some cases, mimic the properties that distinguish neutrino signals from thermal noise. For this reason these contaminated events constitute a background for any neutrino search, although the properties of such signals mean that it is possible to distinguish them from other interesting events. 

CW signals are characterised by large unthermal amplitudes in a small range of frequencies, compared with smaller broadband excesses in impulsive, neutrino-like, signals as power is spread over a larger range of frequencies. This section will describe how the frequency content in the VPol and HPol antenna waveforms is used to identify such events and, as the fraction of these contaminated events is small, remove them from the analysis sample.

Prior to analysis of an event a thermal noise baseline is generated using minimum bias data, being the purest sample of thermal noise available for analysis. This baseline is taken to be a reflection of the thermal conditions in the TestBed during a run. The frequency content of an event can then be compared with this baseline to assess the level of unthermal power and whether this power is confined to a small range of frequencies as expected from CW events.

\subsubsection{Thermal noise baselines}
\label{sec:Analysis:CWRemoval:Baselines}

The frequency domain amplitudes of thermal noise are Rayleigh distributed:

\begin{equation}
  \mbox{Rayleigh p.d.f.} = \frac{A}{\sigma^{2}}e^{\frac{-A^{2}}{2\sigma^{2}}}
  \label{eq:Analysis:CWRemoval:Rayleigh-pdf}
\end{equation}

\noindent where $A$ is the amplitude at frequency $f$ and $\sigma$ can be used to characterise the distribution. When creating a baseline for a run the Fourier transform is taken of the time domain waveforms for each minimum bias event. The resulting frequency domain amplitudes are histogrammed per frequency per antenna and fitted with Rayleigh distributions.


\begin{figure}[htpb]
  \centering
  \includegraphics[width=\mediumfigwidth]{chapters/analysis/RayleighFit-VPol-Ant1-403.7MHz.pdf}
  \caption{Thermal noise amplitudes in VPol antenna 1 at 403.7 MHz for minimum bias data taken on 20th May 2011 (black) and a Rayleigh distribution fit to the data (red). The dashed black line is the same histogram but this time populated from a run containing a known CW source operating at $\sim 403 \mega \hertz$.}
  \label{fig:analysis:CWRemoval:Baselines:Rayleigh-Fit}
\end{figure}



\FigureRef{fig:analysis:CWRemoval:Baselines:Rayleigh-Fit} shows one such Rayleigh fit for the case of a run containing mainly thermal events, and for a run containing a fraction of CW contaminated events. For each run the Rayleigh fit is characterised by a series of $\sigma$ values, which are calculated per frequency per antenna, which are recorded along with the average power per frequency per antenna. Some runs contain a fraction of CW events large enough to distort the Rayleigh distributions with large unthermal tails in the amplitude distribution, making them unusable as a representation of the thermal conditions, and their baselines are rejected as `bad' (the dashed line in \FigureRef{fig:analysis:CWRemoval:Baselines:Rayleigh-Fit} shows such a run, the resulting Rayleigh fit would differ considerably from that derived from the solid line, even though these runs were taken on the same day with similar thermal conditions). 

Due to the narrow band nature of CW signals it is possible to identify bad baselines by looking for spikes in the average power spectra shown in \FigureRef{fig:analysis:CWRemoval:Baselines:Averaged-Power} or $\sigma$ distributions, although the former was found to have better discriminating power. These spikes are identified by looking for the second derivative of the average power spectrum falling below a threshold, indicating the presence of a spike. Any run that contains at least one borehole antenna with a spike in the average power spectrum is rejected. 

\begin{figure}[htpb]
  \subfloat[VPol Antennas]{\includegraphics[width=0.48\textwidth]{chapters/analysis/Rayleigh-Sigma-VPol.pdf}}\hfill
  \subfloat[HPol Antennas]{\includegraphics[width=0.48\textwidth]{chapters/analysis/Rayleigh-Sigma-HPol.pdf}}
  \caption{Rayleigh fit derived $\sigma$ values for two different baselines. The solid lines are for a baseline calculated from a thermal noise sample, and the dashed lines for a baseline containing a known CW source operating at $403 \mega \hertz$.} 
  \label{fig:analysis:CWRemoval:Baselines:Rayleigh-Sigma}
\end{figure}

The baselines calculated in this manner are used to summarise the thermal conditions of the TestBed, examples are shown in \FigureRef{fig:analysis:CWRemoval:Baselines:Rayleigh-Sigma}. The baseline that an event is compared to should therefore be chosen to be from a period with similar thermal conditions. In the main this will be the baseline calculated from the run in which the event was recorded, but where this baseline contains a large amount of CW contamination (hence the baseline is rejected) the closest run in time is used. The variation of baseline calculated $\sigma$ values as a function of time is shown in \FigureRef{fig:analysis:CWRemoval:Baselines:Rayleigh-Sigma}. The thermal conditions change slowly compared with the rate at which runs are taken, hence no scaling is needed to account for variations in the conditions. 

\begin{figure}[htpb]
  \subfloat[VPol Antennas]{\includegraphics[width=0.49\textwidth]{chapters/analysis/Rayleigh-Power-VPol.pdf}}\hfill
  \subfloat[HPol Antennas]{\includegraphics[width=0.49\textwidth]{chapters/analysis/Rayleigh-Power-HPol.pdf}}
  \caption{Averaged power spectra for (solid lines) a run containing largely thermal events, and (dashed lines) a run containing a CW source. Bad runs, such as that summarised by the dashed lines, are identified by spikes in the averaged power spectra characterised by the second derivative falling below a threshold.}
  \label{fig:analysis:CWRemoval:Baselines:Averaged-Power}
\end{figure}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{chapters/analysis/RayleighVsTime295.2MHz.pdf}
  \caption{Rayleigh $\sigma$ values, averaged over a 7 day period, are shown for all antennas at $295.2 \mega \hertz$ for 2011 and 2012.}
  \label{fig:analysis:CWRemoval:Baselines:SigmaVsTime}
\end{figure}


\subsubsection{Identifying CW contaminated events}
\label{sec:Analysis:CWRemoval:CW-Identification}

Each event has it's frequency content considered to identify CW signals. Discrete Fourier transforms are taken of each event's time domain waveforms. Using the Rayleigh distributions from the chosen baseline it is possible to estimate the probability of the measured frequency domain amplitudes. These probabilities are calculated using the Rayleigh $\sigma$ values, which are taken from the relevant baseline, as follows:

\begin{equation}
  P(A \geq A_{meas}) & = \int \limits_{A_{meas}}^{\infty}\frac{A}{\sigma^{2}}e^{\frac{-A^{2}}{2\sigma^{2}}}dA\\
  & = e^{\frac{-A_{meas}^{2}}{2\sigma^{2}}}
  \label{eq:Analysis:CWRemoval:Rayleigh-cdf}
\end{equation}

\noindent where $A_{meas}$ is the measured amplitude of a given frequency for the waveform in question. $P(A \geq A_{meas})$ is then a measure of the probability of thermal fluctuations giving rise to the measured amplitude, and can be interpreted as measuring how `unthermal' that amplitude is. Most CW transmitters have a single polarisation and will be seen in multiple antennas, a feature that can be utilised to improve discrimination. In order to push down the threshold CW signal size to which the filter is sensitive the antennas in each polarisation are grouped together and considered separately. In each polarisation the product of probabilities for the borehole antennas is taken:

\begin{equation}
  P_{prod} = \prod P_{i} = \prod P(A_{i} \geq A_{i, meas})
  \label{eq:Analysis:CWRemoval:Prod-Prob}
\end{equation}

\noindent where $P_{i}$ is the probability for antenna $i$. Any frequencies that have $P_{prod}$ less than some threshold are considered to be in excess. The use of this product was found to have greater sensitivity than various other methods attempted, including taking antennas individually or in combination (for example requiring that 3 out of 4 antennas see an excess at the same frequency).


\begin{figure}[htpb]
  \subfloat[Noise Waveform]{\includegraphics[width=0.49\textwidth]{chapters/analysis/Waveform-VPol-Ant2-Noise-Med.pdf}}\hfill
  \subfloat[Noise Probability Spectrum]{\includegraphics[width=0.49\textwidth]{chapters/analysis/ProdProb-VPol-Noise-Med.pdf}}\\
  \subfloat[CW Contaminated Waveform]{\includegraphics[width=0.49\textwidth]{chapters/analysis/Waveform-VPol-Ant2-WB-Med.pdf}}\hfill
  \subfloat[CW Probability Spectrum]{\includegraphics[width=0.49\textwidth]{chapters/analysis/ProdProb-VPol-WB-Med.pdf}}
  \caption{Example waveforms from VPol antenna 2 showing a thermal noise event and a CW contaminated event. Also shown are the product of probabilities distribution for all VPol antennas for these events. The dashed lines show the two probability thresholds, with any frequencies passing this threshold regarded as being well in excess of thermal noise levels. The upper threshold is to identify non-thermal excesses and the lower to identify broadband signals.}
  \label{fig:analysis:CWRemoval:Baselines:Waveforms}
\end{figure}


Example waveforms from VPol antenna 2 are shown in \FigureRef{fig:analysis:CWRemoval:Baselines:Waveforms} for a thermal and CW contaminated event, along with the corresponding product of probabilities. Any CW removal must be able to reject contaminated events whilst passing broadband signals, such as those induced by neutrino interactions or from calibration sources. Two thresholds are used to identify CW events: one used to identify non-thermal excesses, and a second, lower threshold, to check whether an excess is broadband in nature. The former, which is set such that 1 in $10^{6}$ thermal noise events will be have at least one frequency in excess, is used to identify events that may contain CW. If an event has no frequencies passing this threshold in either polarisation it passes the CW check. Any events that have 1 or more frequencies passing this threshold are then checked using the lower broadband threshold, which is set at a value of $-12.5$, informed by checks against calibration pulser events. If the number of frequencies in excess of this lower threshold, which is referred to as `totalBins', is greater than or equal to 20 then the event is classed as having a broadband signal and is passed. 


In \FigureRef{fig:analysis:CWRemoval:Baselines:Waveforms} the CW contaminated event is clearly identified by a narrow range of frequencies passing both thresholds, and the event is rejected. \FigureRef{fig:analysis:CWRemoval:Filtering:CW-Time} shows how this product of probabilities spectrum changes over the course of a run for calibration events and for non-calibration events. Here the product of probabilities is averaged for all events over 1 minute intervals. Also shown are the same parameter but only for frequencies that are identified as being in excess. A weather balloon launch is clearly visible in these spectra, resulting in a narrow range of frequencies exhibiting highly unthermal amplitudes in the VPol antennas, whereas calibration pulser events show a broad range of frequencies with non-thermal amplitudes.


\begin{figure}[htpb]
  \subfloat[Calibration Pulser Events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CW-Prob-Freq-Time-CP.eps}}\hfill
  \subfloat[Non Calibration Pulser Events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CW-Prob-Freq-Time-NCP.eps}}\\
  \subfloat[Calibration Pulser Events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CW-Prob-Freq-Time-CP-Excess.pdf}}\hfill
  \subfloat[Non Calibration Pulser Events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/CW-Prob-Freq-Time-NCP-Excess.pdf}}
  \caption{The probability spectra for events averaged over 1 minute periods for (a) calibration pulser events (b) for non-calibration pulser events. A weather balloon launch at 23:20 is clearly visible as the turn on of a CW signal at $403 \mega \hertz$ in both figures. The calibration pulser events show a broad range of frequencies having excess power for the duration of the run.}
  \label{fig:analysis:CWRemoval:Filtering:CW-Time}
\end{figure}




In addition to the narrow range of frequencies exhibiting non-thermal power the size of these excesses can be used to identify particularly bright events. A parameter, `minProb', is used to quantify the size of the biggest non-thermal excess in the probability spectra of an event. This parameter is taken to be the minimum value of $\ln(\prod P_{i})$ from the probability spectrum for an event. Since CW signals transmit all of their power in a narrow range of frequencies minProb can take much more negative values (and hence larger unthermal amplitudes) than for broadband impulsive signals. As waveforms are formed of a discrete set of time domain samples the Fourier transform is also comprised of a discrete set of frequencies. As a result particularly large amplitude CW signals can lead to leakage into multiple frequencies, as the actual frequency is not one of those in the Fourier transform. In such a case CW signals can lead to totalBins approaching, or passing, the cut threshold of 20. A cut is placed on minProb to remove such events, removing any event where minProb is less than $-400$. This cut value is safely removed from those observed in calibration pulser events as illustrated in \FigureRef{fig:analysis:CWRemoval:Filtering:CW-MinProb-TotalBins}.

\begin{figure}[htpb]
  \subfloat[Non-calibration pulser events]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CW-minProb-totalBins-NCP.pdf}}\hfill
  \subfloat[Calibration pulser events]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CW-minProb-totalBins-CP.pdf}}\\
  \subfloat[Non-calibration pulser events]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CW-minProb-totalBins-NCPNot403.pdf}}\hfill
  \subfloat[Calibration pulser events]{\includegraphics[width=0.48\textwidth]{chapters/analysis/CW-minProb-totalBins-CPNot403.pdf}}
  \caption{CW parameters minProb (minimum value of $\ln(\prod P_{i})$ in probability spectra) and totalBins (total number of frequency bins that are identified to have non-thermal amplitudes) are shown for calibration pulser and non-calibration pulser events. The top row shows these parameters for the full range of in band frequencies, and the bottom panel excludes a range of frequencies around $403 \mega \hertz$ which are used by the weather balloon. The large vertical tails in the top row are clearly due to the presence of CW signals from the weather balloon and are used to inform cuts on minProb and totalBins.}
  \label{fig:analysis:CWRemoval:Filtering:CW-MinProb-TotalBins}
\end{figure}


\section{Data quality cuts}
\label{sec:Analysis:Data-Quality-Cuts}

A number of data quality cuts were implemented to remove various types of corrupted events. Approximately 1 month of data from 2012 were affected by a digitiser operation issue which resulted in waveforms with fewer than the usual number of time domain samples. These events were not recoverable and were thrown away using a minimum number of samples cut.

Another class of corrupted events were ones that had been corrupted during data processing, resulting in voltage-time waveforms with samples that had their voltage values corrupted.

A final class of bad quality events had a systematic and unphysical ramp up of voltage common to all digitised channels. These events were removed via an algorithm that checked the validity of data from each digitiser's clock channel.


\section{Thermal cuts}
\label{sec:Analysis:Thermal-Cuts}

The analysis of TestBed data aims to remove all thermal events with a small number of cuts derived from the CSW, and CSW reconstruction described in \SectionRef{sec:Analysis:Reconstruction}. As the expected number of candidate neutrino events is very small the cuts implemented must reject all thermal events with a high level of confidence, with the target number of thermal events passing cuts around $0.1$ in the full data sample.

The minimum bias data collected during the analysis period was used as a sample of thermal events. Due to the periodic presence of CW contamination cuts were applied to the thermal sample to remove contaminated events as described in \SectionRef{sec:Analysis:CWRemoval}. 

\subsection{Pseudo-$\chi^{2}$ cut}
\label{sec:Analysis:Thermal-Cuts:Pseudo-ChiSq}

For each event CSW reconstruction is performed using the VPol and HPol antennas' waveforms separately. As described in \SectionRef{sec:Analysis:Reconstruction} a pseudo-$\chi^{2}$ value is calculated for the best fit source location during the reconstruction, and this value is used to measure the quality of reconstruction. The reconstruction in each polarisation is classified as `good' when the best fit pseudo-$\chi^{2}$ value is below a value of two, and the event's polarisation is passed.

\begin{figure}[htpb]
  \subfloat[VPol 2011]{\includegraphics[width=0.48\textwidth]{chapters/analysis/ChiSqVPol.pdf}}\hfill
  \subfloat[HPol 2012]{\includegraphics[width=0.48\textwidth]{chapters/analysis/ChiSqHPol.pdf}}
  \caption{Best fit pseudo-$\chi^{2}$ values for minimum bias, calibration and simulated neutrino events. Events are passed when the pseudo-$\chi^{2}$ falls below the chosen cut value of 2, marked by the orange arrow.}
  \label{fig:Analysis:Thermal-Cuts:Pseudo-ChiSq}
\end{figure}
  
\FigureRef{fig:Analysis:Thermal-Cuts:Pseudo-ChiSq} shows the pseudo-$\chi^{2}$ values for each polarisation for a sample of thermal, calibration pulser and simulated neutrino events. Only $\sim 10^{-7}$ calibration pulser events have a best bit pseudo-$\chi^{2}$ in the same polarisation greater than 1 which was used to inform the chosen cut value.

\subsection{Powherence cut}
\label{sec:Analysis:Thermal-Cuts:Powherence}

Along with being reconstructable neutrino events are expected to exhibit non-thermal levels of power and coherence between antennas, which can be used to separate the two types of event. The peak absolute voltage in the CSW, named `CSWPeakVoltage', is used a measure of the power in the event. 

In order to measure the coherence between antennas a cross-correlation is taken between each antenna and the CSW minus that antenna:

\begin{equation}
  \mbox{CSW}(t) = \frac{1}{N}\sum_{i} \psi_{i}(t+\Delta t_{1,i})\\
  \mbox{CSW}_{N-1}^{j}(t) = \frac{1}{N-1}\sum_{j \neq i} \psi_{i}(t+\Delta t_{1,i})\\
  \mbox{C}_{N-1}^{j}(\Delta t) = {\mbox{CSW}_{N-1}^{j} \star \psi_{j}}
\end{equation}

\noindent where $\psi_{j}(t)$ is the time domain waveform of antenna $j$, CSW$_{N-1}^{j}(t)$ is the CSW minus antenna $j$ and C$_{N-1}^{j}(t)$ is the resulting cross-correlation waveform. For each cross-correlation waveform the maximum correlation value is taken and these values are summed to form a variable `sumCorrVals', which gives a measure of the correlation between antennas in the CSW.

The best discrimination between calibration pulser events (as a proxy for neutrino signals) and thermal noise events was found by taking a linear combination of CSWPeakVoltage and sumCorrVals. The resulting parameter, dubbed `powherence', should have low values for thermal events and high values for signal type events.


\begin{figure}[htpb]
  \subfloat[VPol 2011]{\includegraphics[width=0.48\textwidth]{chapters/analysis/PowHerenceVPol.pdf}}\hfill
  \subfloat[HPol 2012]{\includegraphics[width=0.48\textwidth]{chapters/analysis/PowHerenceHPol.pdf}}
  \caption{The composite parameter powherence is shown for a sample of thermal, calibration pulser and simulated neutrino events after the application of the pseudo-$\chi^{2}$ cut, along with the chosen cut value in orange. Events that have a $\mbox{powherence} > 380$ are passed as being signal like events.}
  \label{fig:Analysis:Thermal-Cuts:Powherence}
\end{figure}

In \FigureRef{fig:Analysis:Thermal-Cuts:Powherence} we see the powherence value calculated in each polarisation for the thermal data set alongside calibration pulser and simulated neutrino events. The relative brightness of the 2011 VPol calibration pulser can be seen clearly in the large values of powherence produced compared with the 2012 HPol calibration pulser.


\section{Anthropogenic cuts}
\label{sec:Analysis:Anthropogenic-Cuts}

After applying thermal noise cuts the analysis sample should contain only signal-like events, however a large fraction (if not all) of these are expected to be associated with human activity and infrastructure at the South Pole. These events are likely to be repetitive both in reconstructed direction and in time, meaning they can be identified and removed by considering associated variables.

The first set of cuts are based on the CSW reconstructed source location and are referred to as `geometry cuts', the second set are to identify noisy periods within the analysis data set and are known as the `goodTimes' cuts.

\subsection{Geometry cuts}
\label{sec:Analysis:Anthropogenic-Cuts:Geometry-Cuts}

A series of conservative geometry cuts are placed on an event's reconstructed source location designed to remove a large fraction of noise events:

\begin{itemize}

\item \textbf{Downward pointing cut} All events that reconstruct to elevation angles above $40^\circ$ are rejected \footnote{RF signals from above ice sources are refracted when incident upon the air-ice boundary. Any events that reconstructs to an elevation angle greater than the critical angle ($\sim 45^{\circ}$) is likely to be above the ice.}

\item \textbf{South Pole infrastructure cut} All events reconstructing within a $50^\circ$ wide region in azimuth associated with the IceCube laboratory and South Pole Station are rejected

\item \textbf{Calibration pulser cut} All events that reconstruct to within $5^\circ$ of any calibration pulser's azimuth are rejected

\item \textbf{Wind turbine cut} All events reconstructing to within $5^\circ$ of the wind turbine's azimuth are rejected.

\end{itemize}

These cuts were informed from the locations of a variety of infrastructure items in the TestBed coordinate system, which are shown in \FigureRef{fig:Analysis:Anthropogenic-Cuts:Geometry-Cuts:VPol-Reco} along with the reconstructed locations of all events passing thermal and CW cuts. The reconstructed positions are clearly not uniform and show excesses close to the chosen cuts.

\begin{figure}[htpb]
  \subfloat[Geometry cuts and infrastructure locations]{\includegraphics[width=0.49\textwidth]{chapters/analysis/TBGeometry.pdf}}\hfill
  \subfloat[VPol events passing thermal cuts]{\includegraphics[width=0.49\textwidth]{chapters/analysis/RecoVPol.pdf}}
  \caption{The location of a variety of infrastructure items and chosen geometry cuts are shown in (a), along with the reconstructed azimuth and elevation angles for all events passing CW and thermal cuts in (b). The shaded regions in (a) indicate the reconstructed directions that are rejected by geometry cuts.}
  \label{fig:Analysis:Anthropogenic-Cuts:Geometry-Cuts:VPol-Reco}
\end{figure}

\subsection{Good times cut}
\label{sec:Analysis:Anthropogenic-Cuts:Good-Times}
The final set of cuts termed `goodTimes' criteria reject days of the year that have large numbers of non-thermal events in an attempt to remove repetitive noise associated with human activity. The months of December and January are masked off, and all events within them rejected, as they correspond to the busiest periods of the summer season. Human activity is at its peak during these times involving frequent use of snow-mobiles, radio communication devices and a plethora of other potential noise sources. In order to identify other `noisy' periods of the year the sample of events passing thermal cuts are used to assess activity levels. The total number of events passing thermal cuts over a three day period is considered. A day is masked off, and all non-thermal events from that day rejected, when the sum of events in that day, the previous and next day passes a threshold of 14. The final cut value was informed from the distribution of events passing thermal cuts in the full analysis sample.

\FigureRef{fig:Analysis:Anthropogenic-Cuts:GoodTimes:Event-Rate} shows the distribution of non-thermal events as a function of day of the year for both 2011 and 2012. For 2011 the numbers of non-thermal events significantly increase around the beginning and end of the year, with limited activity during the quiet austral winter. 2012, on the other hand, exhibits relatively few periods in which there are very few non-thermal events. Future analysis of these events is expected to be able to categorise them and enable cuts to be designed to remove a large fraction of them.

\begin{figure}[htpb]
\subfloat[2011 non-thermal events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/GoodTimes2011.pdf}}\hfill
\subfloat[2012 non-thermal events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/GoodTimes2012.pdf}}
\caption{Shown in blue are the number of events per day passing thermal cuts and in red the subset of these events also passing geometry cuts.}
\label{fig:Analysis:Anthropogenic-Cuts:GoodTimes:Event-Rate}
\end{figure}

\section{Cut results}
\label{sec:Analysis:Cut-Results}

The final analysis cuts and the result of their application to the analysis data sample are detailed in \TableRef{tab:Analysis:Cut-Flow} resulting in no events passing all cuts. The thermal cuts, pseudo-$\chi^{2}$ and powherence, remove the majority of events illustrating that the main background in the TestBed is of a thermal origin. Carrier wave contaminated events appear to make up a large fraction of the remaining events, with daily weather balloon launches expected to contribute a significant number of events year round. 

\begin{table}
  \begin{center}
    \begin{tabular}{ c || c | c | c }
      Total & \multicolumn{3}{c}{ $3.77 \times 10^{8}$}\\
      \hline
      Cut & \multicolumn{3}{c}{ Number passing either polarisation}\\      
      \hline
      Data Quality & \multicolumn{3}{c}{$3.67 \times 10^8$}\\
      pseudo-$\chi^{2}$ & \multicolumn{3}{c}{$3.51 \times 10^8$}\\
      \hline\hline
      & \multicolumn{3}{c}{VPol}\\
      \hline
      cut & passing in order & rejected if last cut & rejected if first cut \\
      \hline
      pseudo-$\chi^{2}$ & $3.49 \times 10^6$ & - & -\\
      powherence & $1.20 \times 10^6$ & $318$ & $2.29 \times 10^6$\\
      CW - totalBins & $4.42 \times 10^5$ & $1.69 \times 10^3$ & $2.37 \times 10^6$\\
      CW - minProb & $1.74 \times 10^5$ & $217$ & $2.12 \times 10^6$\\
      Geometry & $7.12 \times 10^3$ & $318$ & $2.76 \times 10^6$\\
      goodTimes & $0$ & $7.12 \times 10^3$ & $2.81 \times 10^6$\\
      \hline\hline
      & \multicolumn{3}{c}{HPol}\\
      \hline
      cut & passing in order & rejected if last cut & rejected if first cut \\
      \hline
      pseudo-$\chi^{2}$ & $1.20 \times 10^6$ & - & -\\
      powherence & $5.14 \times 10^5$ & $144$ & $6.89 \times 10^5$\\
      CW - totalBins & $4.31 \times 10^5$ & 44 & $4.46 \times 10^5$\\
      CW - minProb & $1.47 \times 10^5$ & $4.47 \times 10^3$ & $1.00 \times 10^6$\\
      Geometry & $9.15 \times 10^3$ & 183 & $9.03 \times 10^5$\\
      goodTimes & $0$ & $9.15 \times 10^3$ & $9.63 \times 10^5$\\      
      \end{tabular} 
    \caption{Analysis cuts applied to the `$90 \%$ sample'.}
    \label{tab:Analysis:Cut-Flow}
  \end{center}
\end{table}

The observation of no candidate neutrino events needs to be placed into context. In order to do so it is necessary to assess two important factors: firstly the efficiency of detecting simulated neutrinos using these cuts and secondly an estimation of the expected number of background events. The background estimation is discussed in \SectionRef{sec:Analysis:Background-Estimation}, and the analysis efficiency in \SectionRef{sec:Analysis:Efficiency}.


\section{Analysis efficiency}
\label{sec:Analysis:Efficiency}

The efficiency of passing candidate neutrino events for each cut individually and in turn is a fundamental factor in calculating an observed flux or limit using the TestBed data. Dedicated simulated neutrino data sets were generated using the official ARA simulation, AraSim, at fixed neutrino energies. The simulation accounts for the interaction of incident neutrinos in the ice, the emission of Askaryan radiation, the propagation of this radiation through the ice and the full TestBed trigger and digitisation chains. As such these data sets are the best representation available of what neutrino signals will look like in the TestBed data set and can be used to test the effectiveness of the cuts developed and described previously.

The analysis passing efficiency is shown in \FigureRef{fig:Analysis:Cut-Results:Efficiency} as a function of neutrino energy and also signal to noise ratio (SNR), which is calculated as follows:

\begin{equation}
  \mbox{SNR} = \frac{\lvert V_{peak} \rvert}{\mbox{RMS}}
\end{equation}

\noindent where $\lvert V_{peak} \rvert$ is the absolute value of the peak voltage of each waveform. SNR values are calculated for all antennas and the smallest value in each polarisation compared, the larger of the two is then used as a measure of signal strength (as the RF impulse in events can have an arbitrary polarisation angle, so the brighter of the two polarisations is used).

\begin{figure}[htpb]
\subfloat[Analysis efficiency as a function of neutrino energy]{\includegraphics[width=0.49\textwidth]{chapters/analysis/efficiencyEnergy.pdf}}\hfill
\subfloat[Analysis efficiency as a function of SNR]{\includegraphics[width=0.49\textwidth]{chapters/analysis/efficiencySNR.pdf}}
\caption{Analysis efficiency for simulated neutrino events. The solid black line shows the efficiency after applying all cuts and the dashed lines show efficiencies for individually applied cuts.}
\label{fig:Analysis:Cut-Results:Efficiency}
\end{figure}


The efficiencies in \FigureRef{fig:Analysis:Cut-Results:Efficiency} are calculated for application of cuts individually (dashed lines) and for all cuts applied. The goodTimes cut is not accounted for as it acts as a scaling of the detector live time. The overall analysis efficiency ranges between $15 \%$ and $30 \%$ across the range of energies simulated, whereas it peaks at $60 \%$ for moderately large values of SNR. Two factors influence the low efficiency as a function of neutrino energy: firstly the simulation produces large numbers of events with very small SNR which mainly consist of upward fluctuations in the thermal noise, and secondly compression in the digitisation chain leads to a drop off in efficiency at high SNR. 

The drop off in efficiency at high SNR is driven by very large frequency domain amplitudes being produced when particularly bright signals are propagated through the simulated signal chain. As a result the CW minProb cut removes a large fraction of these very bright events. A possible cause for this effect is incomplete modelling of compression in the signal chain. When large voltages are input to amplifiers in the signal chain they can push them into a non-linear mode of operation, causing smaller amplification of large voltages than small ones. It was found that the simulation does not account for this effect in the manner observed in bright events in the TestBed data set, for example the 2011 VPol calibration pulser. 

\begin{figure}[hptb]
  \includegraphics[width=\largefigwidth]{chapters/analysis/cutFlow.pdf}
  \caption{The passing efficiency of analysis cuts applied in turn for various data types. The data types are: calibration pulser events (blue), simulated neutrino events (grey), non-calibration pulser events in the `$90 \%$ sample' (green) and minimum bias events from the `burn sample' (red).}
  \label{fig:Analysis:Cut-Results:CutFlow}
\end{figure}

The cumulative effect on passing efficiency is shown for a variety of data types in \FigureRef{fig:Analysis:Cut-Results:CutFlow}. The largest drop in efficiency for passing calibration pulser events comes from the data quality issues suffered for $\sim$ 1 month in 2012, affecting $5.8 \%$ of these events. Of the events not suffering data quality issues the analysis is $96.3 \%$ efficient up to the application of geometry and good times cuts, the first of which specifically removes events reconstructing to the calibration pulser locations.

\section{Background estimation}
\label{sec:Analysis:Background-Estimation}

Due to the differences in signal type it is necessary to estimate backgrounds separately for thermal noise and anthropogenic events. In order to estimate the expected number of events that pass all cuts the distributions for events failing particular cuts, specific to signal type, were considered and extrapolated from fits to data.

For thermal noise events the value of powherence was considered the most relevant parameter and proved to follow closely an exponentially falling distribution. Two samples of events passing all but the powherence cuts were assessed: the first being all minimum bias events and the second all non-calibration pulser events, in both cases pseudo-$\chi^{2}$ and anthropogenic cuts are applied. The former sample of events is the purest sample of thermal noise since no trigger condition is met for these events, however a scaling must be applied when accounting for the increase in statistics between this data set and the full set of non-calibration pulser events. The distribution of powherence in VPol and HPol and fits to the falling edge approaching the cut value are shown in \FigureRef{fig:Analysis:Background-Estimation:Thermal-Background} for the minimum bias sample. 

\begin{figure}[htpb]
\subfloat[VPol minimum bias events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/PowHerenceFitVPol.pdf}}\hfill
\subfloat[HPol minimum bias events]{\includegraphics[width=0.49\textwidth]{chapters/analysis/PowHerenceFitHPol.pdf}}
\caption{Thermal noise sample taken from the minimum bias data set is fitted to an exponential function for VPol and HPol events. This is then used to extrapolate beyond the chosen cut value (orange line) to estimate the expected number of thermal events passing the cut in the analysis data sample.}
\label{fig:Analysis:Background-Estimation:Thermal-Background}
\end{figure}

Extrapolating the fits to each of the thermal samples of events provides two estimates for the number of events passing the powherence cut and errors associated with the fits themselves. Since fits to both samples were in good agreement to data the average of the two was taken as the estimate for the background, with the error taken to be the difference between the two. The result is an expected number of background events of a thermal origin to be $0.33^{+0.15}_{-0.15}$.

For the case of anthropogenic signals it is much more difficult to ascertain the expected background levels since these events are from an unidentified mixture of sources, which are not necessarily expected to follow well described distributions. In order to estimate this type of background some of the geometric cuts, designed to remove anthropogenic signals, were relaxed to see whether events failing these cuts could be used to extrapolate past the cut values. \FigureRef{fig:Analysis:Background-Estimation:Anthropogenic-Background} shows events failing the geometric cut around the azimuthal direction of the IceCube Laboratory and South Pole Station.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{chapters/analysis/BackgroundGeometry.pdf}
  \caption{VPol events failing the IceCube Laboratory and South Pole geometry cut via their reconstructed azimuth. The geometric cut is indicated by the orange line at $-150$.}
  \label{fig:Analysis:Background-Estimation:Anthropogenic-Background}
\end{figure}

A fit was made to the azimuthal distribution of VPol events failing the geometric cut and extrapolated past the cut point at $-150$. The same procedure was attempted with the results of HPol reconstruction and cuts, but proved difficult to extrapolate so the estimate for VPol events was also used for HPol. The result of the anthropogenic background estimation was $0.44^{+0.60}_{-0.32}$. Taken in combination with the estimation of thermal backgrounds, the final estimate for the number of background events passing analysis cuts was $0.77^{+0.31}_{-0.43}$.


